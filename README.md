# AfterQuery RL-Style Evaluation Environment

A reproducible RL-style evaluation harness for measuring how well LLMs act as autonomous agents to create working "0â†’1" web applications from product promptsâ€”covering planning, coding, debugging, running, and shipping.

## Overview

This environment evaluates LLM agents on their ability to:
- Plan implementation from requirements
- Write clean, functional code
- Debug and fix errors
- Build and deploy working applications
- Pass automated checks and quality reviews

## Quick Start

### Prerequisites

- **Python 3.10+**
- **Node.js 18+** and **pnpm**
- **API Keys**: OpenAI, Anthropic, or Google AI

### Installation

1. **Clone the repository:**
```bash
git clone <repo-url>
cd RL-environment
```

2. **Install Python dependencies:**
```bash
pip install -e .
```

This installs the project in editable mode from `pyproject.toml`.

3. **Set up environment variables:**
Create a `.env` file:
```bash
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
GOOGLE_API_KEY=...
```

4. **Install Node.js tools:**
```bash
npm install -g pnpm
```

### Run Your First Episode

**Simple test:**
```bash
python3 env/runner.py "Build a counter app with increment and decrement buttons"
```

**With HuggingFace dataset:**
```bash
python3 env/runner.py --data data/prompts.csv --row-index 0
```

**With specific model:**
```bash
python3 env/runner.py --data data/prompts.csv --model claude-sonnet
```

## Usage

### Command-Line Interface

```bash
python3 env/runner.py [OPTIONS] [TASK]
```

**Options:**
- `--data PATH`: Path to CSV file with HuggingFace dataset format
- `--row-index N`: Row index to load from CSV (default: 0)
- `--model NAME`: Model identifier from `configs/models.yaml` (default: gemini-flash)
- `--template NAME`: Template from `templates/` directory (default: nextjs-starter)
- `--max-steps N`: Maximum agent steps (default: 50)
- `--quiet`: Disable verbose logging

### Examples

**1. Run with HuggingFace dataset:**
```bash
python3 env/runner.py --data data/prompts.csv --row-index 0
```

**2. Test different models:**
```bash
# Gemini 2.0 Flash (fast, default)
python3 env/runner.py --data data/prompts.csv --model gemini-flash

# Claude Sonnet 4.5 (best for coding)
python3 env/runner.py --data data/prompts.csv --model claude-sonnet

# GPT-4o-mini (fast and cheap)
python3 env/runner.py --data data/prompts.csv --model gpt-4o-mini
```

**3. Custom task with specific settings:**
```bash
python3 env/runner.py \
  "Build a habit tracker with daily check-ins" \
  --model claude-sonnet \
  --max-steps 30
```

**4. Run multiple episodes:**
```bash
# Evaluate models on first 5 tasks
for i in {0..4}; do
  python3 env/runner.py --data data/prompts.csv --row-index $i --model gemini-flash
done
```

## Dataset Format

The system expects a CSV file with the following columns (from [AfterQuery/App-Bench](https://huggingface.co/datasets/AfterQuery/App-Bench)):

| Column | Description |
|--------|-------------|
| `App Name` | Name of the application |
| `App Description` | Brief description |
| `Prompt` | Main product requirements |
| `Addition for CLI Tools` | Technical constraints (e.g., "Use Supabase") |
| `Rubric` | Grading criteria for LLM judge |

**Example:**
```csv
App Name,App Description,Prompt,Addition for CLI Tools,Rubric
Stock Tracker,Track stock portfolio,"Build a stock portfolio tracker...","Use localStorage for data","Evaluate on: 1) Functionality..."
```

## Architecture

### Components

#### 1. **Agent Harness** (`env/`)
Manages the episode lifecycle and provides tools to the agent:

**Tools:**
- `write_file(path, content)` - Create/modify files
- `read_file(path)` - Read file contents
- `run_command(cmd, cwd)` - Execute shell commands
- `install_deps()` - Install Node.js dependencies with architecture detection
- `start_server()` - Start development server in background
- `finish_task()` - Signal task completion

**Files:**
- `runner.py` - Episode orchestration
- `tools.py` - Tool implementations
- `sandbox.py` - Process isolation and execution

#### 2. **Agent** (`agent/`)
LLM-powered autonomous coding agent using ReAct pattern:

**Strategy:**
- Thinks about the task
- Plans next action
- Executes tools
- Reflects on results
- Repeats until complete

**Files:**
- `react_agent.py` - ReAct agent implementation
- `prompts/system.txt` - System prompt with instructions
- `prompts/tool_schema.json` - Tool definitions for LLM

#### 3. **Grader** (`grader/`)
Hybrid evaluation with automated checks + LLM judge:

**Automated Checks:**
- âœ… Dependencies install successfully
- âœ… Application builds without errors
- âœ… Server starts and responds with HTTP 200

**LLM Judge:**
- ğŸ“Š Code quality and clarity
- ğŸ¨ UI/UX polish
- ğŸ¯ Product requirements fit
- ğŸ›¡ï¸ Edge case handling

**Files:**
- `grade.py` - Automated checks (install, build, server health)
- `rubric_judge.py` - LLM-based code evaluation

### Episode Lifecycle

```
1. Init
   â””â”€ Create fresh workspace from template

2. Agent Loop
   â”œâ”€ Read task prompt + workspace state
   â”œâ”€ Think and plan next action
   â”œâ”€ Execute tool calls
   â”œâ”€ Implement features
   â”œâ”€ Run tests and fix errors
   â””â”€ Repeat until done or max steps

3. Grading
   â”œâ”€ Run install_deps()
   â”œâ”€ Run build (pnpm build)
   â”œâ”€ Check server health (pnpm start)
   â”œâ”€ Run LLM judge on code
   â””â”€ Generate grade.json

4. Output
   â”œâ”€ runs/<timestamp>/workspace/  (agent's work)
   â”œâ”€ runs/<timestamp>/result.json (episode data)
   â””â”€ runs/<timestamp>/grade.json  (grading results)
```

## Repository Structure

```
RL-environment/
â”œâ”€â”€ README.md
â”œâ”€â”€ pyproject.toml           # Python dependencies
â”œâ”€â”€ .env                     # API keys (gitignored)
â”‚
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ models.yaml          # Model configurations
â”‚
â”œâ”€â”€ env/                     # Agent Harness
â”‚   â”œâ”€â”€ runner.py            # Episode orchestration
â”‚   â”œâ”€â”€ tools.py             # Tool implementations
â”‚   â””â”€â”€ sandbox.py           # Process isolation
â”‚
â”œâ”€â”€ agent/                   # Autonomous Agent
â”‚   â”œâ”€â”€ react_agent.py       # ReAct agent logic
â”‚   â””â”€â”€ prompts/
â”‚       â”œâ”€â”€ system.txt       # System prompt
â”‚       â””â”€â”€ tool_schema.json # Tool definitions
â”‚
â”œâ”€â”€ grader/                  # Evaluation System
â”‚   â”œâ”€â”€ grade.py             # Automated checks
â”‚   â””â”€â”€ rubric_judge.py      # LLM judge
â”‚
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ nextjs-starter/      # Next.js 16 + Tailwind v3
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ prompts.csv          # HuggingFace dataset
â”‚
â””â”€â”€ runs/                    # Episode outputs
    â””â”€â”€ <timestamp>/
        â”œâ”€â”€ workspace/       # Agent's code
        â”œâ”€â”€ result.json      # Episode results
        â””â”€â”€ grade.json       # Grading scores
```

## Configuration

### Adding New Models

Edit `configs/models.yaml`:

```yaml
models:
  your-model-name:
    model_name: "provider/model-id"
    litellm_params:
      model: "provider/model-id"
      temperature: 0.7
```

Supported providers via LiteLLM:
- OpenAI: `gpt-4o`, `gpt-4o-mini`
- Anthropic: `anthropic/claude-sonnet-4-5-20250929`
- Google: `gemini/gemini-2.0-flash-001`
- OpenRouter: `openrouter/model-name`

**Current models in `configs/models.yaml`:**
- `gpt-4o-mini` - OpenAI GPT-4o-mini
- `claude-sonnet` - Anthropic Claude Sonnet 4.5
- `gemini-flash` - Google Gemini 2.0 Flash (default)

### Mock Mode (Isolated Environment)

The system runs agents in **Mock Mode** to avoid requiring external services:

**Automatically mocked:**
- ğŸ—„ï¸ Databases (Supabase, Firebase) â†’ Use localStorage or JSON files
- ğŸ” Authentication â†’ Mock login system
- ğŸ’³ Payment APIs (Stripe) â†’ Fake success responses
- ğŸ“Š External APIs (Stock data, Weather) â†’ Random/mock data

This is automatically injected via the prompt, so agents build functional UIs without real API keys.

## Testing Components

### Test the Grader

```bash
# Test automated checks only
python3 test_grader.py runs/<timestamp>/workspace

# Test with server health check (slower)
python3 test_grader.py runs/<timestamp>/workspace --all
```

### Test the LLM Judge

```bash
python3 test_judge.py runs/<timestamp>/workspace
```

## Output & Results

After each episode, results are saved to `runs/<timestamp>/`:

### `result.json`
Complete episode data:
```json
{
  "episode_dir": "runs/20241223_143022",
  "model": "gemini-flash",
  "app_name": "Stock Portfolio Tracker",
  "task": "Build a stock tracker...",
  "agent_result": {
    "success": true,
    "steps": 12,
    "actions": [...]
  },
  "grade_result": {...}
}
```

### `grade.json`
Grading results:
```json
{
  "automated_checks": {
    "install": true,
    "build": true,
    "server_health": true,
    "overall_pass": true
  },
  "llm_evaluation": {
    "score": 85,
    "reasoning": "Clean implementation with good UI...",
    "breakdown": {
      "Functionality": 90,
      "Code Quality": 85,
      "UI/UX": 80,
      "Production Readiness": 85
    }
  },
  "overall_score": 85,
  "overall_pass": true
}
```

### Console Output

```
======================================================================
                         EPISODE SUMMARY
======================================================================

ğŸ“± APP: Stock Portfolio Tracker

ğŸ“‹ AGENT PERFORMANCE
----------------------------------------------------------------------
  Status:       âœ… SUCCESS
  Steps:        12/50
  Model:        gemini-flash

ğŸ”§ AUTOMATED CHECKS
----------------------------------------------------------------------
  Install:      âœ… PASS
  Build:        âœ… PASS
  Server:       âœ… PASS

âš–ï¸  LLM EVALUATION
----------------------------------------------------------------------
  Score:        85/100
  Breakdown:
    - Functionality: 90
    - Code Quality: 85
    - UI/UX: 80
    - Production Readiness: 85

ğŸ† OVERALL RESULT
----------------------------------------------------------------------
  Status:       âœ… PASS
  Final Score:  85/100

ğŸ“‚ OUTPUT
----------------------------------------------------------------------
  Episode:      runs/20241223_143022
  Workspace:    runs/20241223_143022/workspace
  Results:      runs/20241223_143022/result.json
  Grade:        runs/20241223_143022/grade.json
```

## Troubleshooting

### Common Issues

**1. "ModuleNotFoundError"**
```bash
# Use python3 explicitly
python3 env/runner.py --data data/prompts.csv
```

**2. "Cannot find module lightningcss"**
- Fixed! We downgraded to Tailwind CSS v3 to avoid native binary issues

**3. "Server failed to start"**
- Check if port 3000 is already in use: `lsof -ti:3000 | xargs kill -9`
- Verify `pnpm` is installed: `npm install -g pnpm`

**4. API key not found**
```bash
# Ensure .env file exists with your keys
echo "OPENAI_API_KEY=sk-..." > .env
```

**5. Agent gets stuck in loop**
- Reduce `--max-steps` to force earlier termination
- Check `runs/<timestamp>/workspace` to see what agent built

## Development

### Project Structure

The codebase follows a clean architecture:

- **Harness** (`env/`) - Provides tools and manages episodes
- **Agent** (`agent/`) - LLM controller with ReAct loop
- **Grader** (`grader/`) - Automated + LLM evaluation
- **Configs** (`configs/`) - Model settings
- **Templates** (`templates/`) - Starting codebases

### Adding New Tools

1. Add tool to `env/tools.py`:
```python
def your_tool(self, arg: str) -> ToolResult:
    """Tool description."""
    # Implementation
    return ToolResult(success=True, data={...})
```

2. Update `agent/prompts/tool_schema.json`:
```json
{
  "name": "your_tool",
  "description": "What it does",
  "parameters": {...}
}
```